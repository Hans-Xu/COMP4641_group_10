{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from bs4 import BeautifulSoup\n",
    "import plotly.graph_objs as go\n",
    "import chart_studio.plotly as py\n",
    "import cufflinks\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import plotly.figure_factory as ff\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from plotly.offline import iplot\n",
    "cufflinks.go_offline()\n",
    "cufflinks.set_config_file(world_readable=True, theme='pearl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'quoted_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'quoted_text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3d1236e8ceb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#input of the model will be text + quoted_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quoted_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quoted_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'quoted_text'"
     ]
    }
   ],
   "source": [
    "#training set file\n",
    "df = pd.read_csv('/Users/e5pianos/Desktop/2020 SPRING/COMP4641/project/lt_hydrator/lt_oid_info_train.csv',dtype=str,encoding='unicode_escape')\n",
    "df = df[pd.notnull(df['label'])]\n",
    "df = df[pd.notnull(df['text'])]\n",
    "#testing set file (prediction)\n",
    "dft = pd.read_csv('/Users/e5pianos/Desktop/2020 SPRING/COMP4641/project/lt_hydrator/lt_oid_info_test.csv',dtype=str,encoding='unicode_escape')\n",
    "dft = dft[pd.notnull(dft['text'])]\n",
    "\n",
    "#input of the model will be text + quoted_text\n",
    "df['full_text']=df['text'].fillna('') + ' ' + df['quoted_text'].fillna('')\n",
    "dft['full_text']=dft['text'].fillna('') + ' ' + dft['quoted_text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x288 with 0 Axes>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1494da978>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    512\n",
      "1.0    259\n",
      "2.0    179\n",
      "3.0     49\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#statstic of classes\n",
    "my_tags = ['0','1','2','3']\n",
    "plt.figure(figsize=(10,4))\n",
    "df.label.value_counts().plot(kind='bar');\n",
    "print(df.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'full_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8c8c8f75bc63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_text'"
     ]
    }
   ],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "\n",
    "df['full_text'] = df['full_text'].apply(clean_text)\n",
    "dft['full_text'] = dft['full_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['full_text'].apply(lambda x: len(x.split(' '))).sum()\n",
    "dft['full_text'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.full_text\n",
    "y_train=df.label\n",
    "X_test=dft.full_text\n",
    "y_test=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Naive Bayes classifier for multinomial models\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "#perdicting\n",
    "        \n",
    "idx=0\n",
    "yl=[]\n",
    "for i in X_test:\n",
    "    yl.append(y_pred[idx])\n",
    "    idx=idx+1\n",
    "    \n",
    "#CHANGE to your test files directory\n",
    "df=pd.read_csv(\"./lt_hydrator/lt_oid_info_test.csv\",dtype=str,encoding='unicode_escape')\n",
    "df['label']=yl\n",
    "df.to_csv( \"./lt_hydrator/lt_oid_info_predicted.csv\", index=False, encoding='utf-8-sig')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########Linear support vector machine\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import sklearn\n",
    "\n",
    "sgd = sklearn.pipeline.Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "#perdicting\n",
    "        \n",
    "idx=0\n",
    "yl=[]\n",
    "for i in X_test:\n",
    "    yl.append(y_pred[idx])\n",
    "    idx=idx+1\n",
    "    \n",
    "#CHANGE to your test files directory\n",
    "df=pd.read_csv(\"./lt_hydrator/lt_oid_info_test.csv\",dtype=str,encoding='unicode_escape')\n",
    "df['label']=yl\n",
    "df.to_csv( \"./lt_hydrator/lt_oid_info_predicted.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########Logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "#predicting\n",
    "idx=0\n",
    "yl=[]\n",
    "for i in X_test:\n",
    "    yl.append(y_pred[idx])\n",
    "    print(y_pred[idx])\n",
    "    idx=idx+1\n",
    "    \n",
    "#CHANGE to your test files directory\n",
    "df=pd.read_csv(\"./lt_hydrator/lt_oid_info_test.csv\",dtype=str,encoding='unicode_escape')\n",
    "df['label']=yl\n",
    "df.to_csv( \"./lt_hydrator/lt_oid_info_predicted.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Word2vec embedding and Logistic Regression\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import models\n",
    "import gensim\n",
    "\n",
    "#CHANGE to your library directory\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"/Users/e5pianos/Desktop/2020 SPRING/COMP4641/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "list(islice(wv.vocab, 13030, 13050))\n",
    "import nltk\n",
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "train=df\n",
    "test=dft\n",
    "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['full_text']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['full_text']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(X_train_word_average, train['label'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "\n",
    "#predicting\n",
    "idx=0\n",
    "yl=[]\n",
    "for i in X_test_word_average:\n",
    "    yl.append(y_pred[idx])\n",
    "    print(y_pred[idx])\n",
    "    idx=idx+1\n",
    "    \n",
    "#CHANGE to your test files directory\n",
    "df=pd.read_csv(\"./lt_hydrator/lt_oid_info_test.csv\",dtype=str,encoding='unicode_escape')\n",
    "df['label']=yl\n",
    "df.to_csv( \"./lt_hydrator/lt_oid_info_predicted.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###BOW with keras\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "\n",
    "\n",
    "train_posts = df['full_text']\n",
    "train_tags = df['label']\n",
    "\n",
    "test_posts = dft['full_text']\n",
    "test_tags = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize.fit_on_texts(train_posts) # only fit on train\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "\n",
    "text_labels = encoder.classes_ \n",
    "\n",
    "\n",
    "yl=[]\n",
    "\n",
    "for i in x_test:\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    yl.append(predicted_label)\n",
    "    #print(\"Predicted label: \" + predicted_label + \"\\n\")\n",
    "    \n",
    "    \n",
    "#CHANGE to your test files directory\n",
    "df=pd.read_csv(\"./lt_hydrator/lt_oid_info_test.csv\",dtype=str,encoding='unicode_escape')\n",
    "df['label']=yl\n",
    "df.to_csv( \"./lt_hydrator/lt_oid_info_predicted.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Doc2vec and Logistic Regression\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import doc2vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
    "    return labeled\n",
    "\n",
    "X_train, X_test, y_train, y_test = df.full_text,df_label,dft_full_text,dft_label\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "idx=0\n",
    "yl=[]\n",
    "for i in test_vectors_dbow:\n",
    "    yl.append(y_pred[idx])\n",
    "    print(y_pred[idx])\n",
    "    idx=idx+1\n",
    "    \n",
    "#CHANGE to your test files directory\n",
    "df=pd.read_csv(\"./lt_hydrator/lt_oid_info_test.csv\",dtype=str,encoding='unicode_escape')\n",
    "df['label']=yl\n",
    "df.to_csv( \"./lt_hydrator/lt_oid_info_predicted.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
